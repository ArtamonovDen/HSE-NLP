{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14df467c-3619-4e41-aed6-99529ebc1d47",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2: Распознавание именованых сущностей (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618d0a6-efcc-4c41-8f72-39e803c26a06",
   "metadata": {},
   "source": [
    "**Выполнил: Артамонов Д.С, 20 МАГ ИАД**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a798bad6-75ba-4ccb-99d8-76515fa18efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torchtext.datasets import SequenceTaggingDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00ef20-1aca-4e37-983b-779248379e49",
   "metadata": {},
   "source": [
    "Дополнительные пакеты, которые не входят в `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565cd23-64c7-48bd-af74-fe5c26c123e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext==0.6.0 nerus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8459f1-def8-4d53-b992-a502105f74c8",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать [Nerus](https://github.com/natasha/nerus) - почти 700к статей из Ленты.ру, собранных для проекта [Natasha](https://github.com/natasha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9763a-a5c5-4079-8571-c19a25f361cd",
   "metadata": {},
   "source": [
    "## Датасет Nerus и дары torch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba8e9e-2a5e-40a8-b03c-95ed7d322c0e",
   "metadata": {},
   "source": [
    "Датасет загружается отедльно архивчиком: [link](https://github.com/natasha/nerus#:~:text=Download-,nerus_lenta.conllu.gz,-~2GB%2C%20~700K%20texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea819a-ff00-4005-8355-95d310e68c42",
   "metadata": {},
   "source": [
    "Внтури что-то такое:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcaf63-1a95-4f0b-a0a7-def85fb2eff5",
   "metadata": {},
   "source": [
    "```bash\n",
    "friday@fridaydevice:~/HSE/HSE-NLP$ gunzip -c nerus_lenta.conllu.gz | head\n",
    "# newdoc id = 0\n",
    "# sent_id = 0_0\n",
    "# text = Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, сообщает РИА Новости.\n",
    "1\tВице-премьер\t_\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t7\tnsubj\t_\tTag=O\n",
    "2\tпо\t_\tADP\t_\t_\t4\tcase\t_\tTag=O\n",
    "3\tсоциальным\t_\tADJ\t_\tCase=Dat|Degree=Pos|Number=Plur4amod\t_\tTag=O\n",
    "4\tвопросам\t_\tNOUN\t_\tAnimacy=Inan|Case=Dat|Gender=Masc|Number=Plur\t1\tnmod\t_\tTag=O\n",
    "5\tТатьяна\t_\tPROPN\t_\tAnimacy=Anim|Case=Nom|Gender=Fem|Number=Sing\t1\tappos\t_\tTag=B-PER\n",
    "6\tГоликова\t_\tPROPN\t_\tAnimacy=Anim|Case=Nom|Gender=Fem|Number=Sing\t5\tflat:name\t_\tTag=I-PER\n",
    "7\tрассказала\t_\tVERB\t_\tAspect=Perf|Gender=Fem|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\t0\troot\t_\tTag=O\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19012e6-f504-40c2-86ee-acec3432fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NERUS = './nerus_lenta.conllu.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471beb6-50c5-4696-9868-5f6c16386e3f",
   "metadata": {},
   "source": [
    "Авторы датасета сделали удобное API для работы с этим файликом, импортируем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a486bec4-c60c-445c-8500-38833be648cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerus import load_nerus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480c1d2-32c4-4385-8b8e-bd22cacb6472",
   "metadata": {},
   "source": [
    "А ещё импортируем магию RusVectores, которую мы будем использовать для стэмминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ab3242-54db-40b1-b8bd-eed24e103f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "import webvectors.preprocessing.rus_preprocessing_udpipe as udpipe_preproc # cloned from https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_udpipe.py\n",
    "\n",
    "UDPIPE_MODEL = 'udpipe_syntagrus.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1da9c3b-c92c-4582-8323-19345d325e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer:\n",
    "    def __init__(self,modelfile):\n",
    "        self.stemming_model = Model.load(modelfile)\n",
    "        self.process_pipeline = Pipeline(self.stemming_model, 'tokenize',Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        \n",
    "    def stem_word(self, word):\n",
    "        return udpipe_preproc.process(\n",
    "            self.process_pipeline, text=udpipe_preproc.unify_sym(word), keep_punct=True, keep_pos=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67f3e0-e4a3-4bce-b957-b7f22352a4f6",
   "metadata": {},
   "source": [
    "> Пример стеминга"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f889428-d983-4045-9d82-d58c673e8afc",
   "metadata": {},
   "source": [
    "С помощью API можно загрузить датасет в одно строчку и получить итератор, который будет отдавать статьи и их разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841273de-8b08-41e2-a262-455eec3a2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_nerus(NERUS)\n",
    "doc = next(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb21c0e-a83a-4815-86dc-26a5d51fd65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вице-премьер -> ['вице-премьер']\n",
      "по -> ['по']\n",
      "социальным -> ['социальный']\n",
      "вопросам -> ['вопрос']\n",
      "Татьяна -> ['татьяна']\n",
      "Голикова -> ['голикова']\n",
      "рассказала -> ['рассказывать']\n",
      ", -> [',']\n",
      "в -> ['в']\n",
      "каких -> ['какой']\n"
     ]
    }
   ],
   "source": [
    "docs = load_nerus(NERUS)\n",
    "doc = next(docs)\n",
    "stemmer = Stemmer(UDPIPE_MODEL)\n",
    "for i in range(10):\n",
    "    text = doc.sents[0].tokens[i].text\n",
    "    res = udpipe_preproc.process(stemmer.process_pipeline, text=udpipe_preproc.unify_sym(text), keep_punct=True, keep_pos=False) \n",
    "    print(f\"{text} -> {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297852b-b9be-4d2d-8d0a-e1ae41c511b1",
   "metadata": {},
   "source": [
    "Возьмём кусочек датасета и распилим его на train/test/validation и запишем их в `tsv` файлики, а ещё применим стэмминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d24d38f-2fba-4a46-8d8e-0c71468b5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_dataset(path, stemmer, train_num = 5_000, val_num = 500, test_num = 1000):\n",
    "    docs = load_nerus(path)\n",
    "    for num, file in [(train_num, f\"ner_train_{train_num}.tsv\"), (val_num,  f\"ner_val_{val_num}.tsv\"), (test_num,  f\"ner_test_{test_num}.tsv\")]:\n",
    "        with open (file, 'w') as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            while (num):\n",
    "                doc = next(docs)\n",
    "                for sent in doc.sents:\n",
    "                    for token in sent.tokens:\n",
    "                        stem_word = stemmer.stem_word(token.text)\n",
    "                        if stem_word:\n",
    "                            # ignore symbols, emojies and foereign languages \n",
    "                            text = stem_word[0]\n",
    "                            tsv_writer.writerow(\n",
    "                                [text,  token.tag]\n",
    "                            )\n",
    "                        \n",
    "                tsv_writer.writerow([]) # empty line between documents\n",
    "                num -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65127c4-762d-4391-a0ae-8b65b0e28017",
   "metadata": {},
   "source": [
    "Запускаем..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa18bce-bae3-4642-92c1-af14c52def22",
   "metadata": {},
   "source": [
    "( или не запускаем, потому что долго)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc07d679-f48c-4888-be89-6dfed0e474bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = Stemmer(UDPIPE_MODEL)\n",
    "# dump_dataset(path=NERUS, stemmer=stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683049f9-8af8-46c1-98cb-12810d3b83a5",
   "metadata": {},
   "source": [
    "А теперь применим магию TorchText, чтобы сделать удобный DataLoader. В старой версии TorchText есть удобный `SequenceTaggingDataset`, который сделает нам из файликов DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238ee129-daf1-4aec-b19f-7956885b4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_field = Field(lower=True)\n",
    "tag_field = Field(unk_token=None)\n",
    "train_data, val_data, test_data = SequenceTaggingDataset.splits(\n",
    "    path='.',\n",
    "    train='ner_train_5000.tsv',\n",
    "    validation='ner_val_500.tsv',\n",
    "    test='ner_test_1000.tsv',\n",
    "    fields=(('word', word_field),  ('tag', tag_field))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9f153-509b-401d-98e9-294468d45b9b",
   "metadata": {},
   "source": [
    "Каждый элемент датасетов представляет собой 2 списка: \n",
    "* список с предобработанными словами статьи (и знакам пунктуации)\n",
    "* список тегов, соответствующих им\n",
    "\n",
    "При этом каждый элемент - отдельная новостная статья из датасета: `SequenceTaggingDataset` умеет разбивать по пустой строке, которую мы оставили между статьями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3912604a-34fe-4505-8aad-1a5b1b02fd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['жительница', 'американский', 'город', 'оуингс', 'милс', ',', 'штат', 'мэриленд', ',', 'выигрывать', 'джекпот', 'в', 'несколько', 'тысяча', 'доллар', '.', 'как', 'сообщать', 'информационный', 'портал', 'upi', ',', 'она', 'благодарить', 'за', 'это', 'бессонница', '.', 'работать', 'медсестра', '72-летняе', 'женщина', 'решать', 'ложиться', 'рано', 'перед', 'долгий', 'смена', 'в', 'больница', ',', 'но', 'в', 'она', 'не', 'выходить', '.', '\"\"\"\"', 'я', 'постоянно', 'смотреть', 'на', 'час', 'и', 'беситься', ',', 'что', 'никак', 'не', 'мочь', 'засыпать', '.', 'последний', 'время', ',', 'который', 'я', 'запомнить', ',', 'быть', 'xx', ':', 'xx', 'вечер', '\"\"\"\"', ',', '-', 'рассказывать', 'американка', '.', 'по', 'она', 'слово', ',', 'этот', 'число', 'запасть', 'она', 'в', 'голова', ',', 'поэтому', 'на', 'следующий', 'после', 'смена', 'день', 'она', 'отправляться', 'в', 'магазин', 'за', 'лотерейный', 'билет', '.', 'женщина', 'решать', 'поиграть', 'в', 'виртуальный', 'скачка', 'и', 'ставить', 'на', 'компьютерный', 'лошадь', 'под', 'номер', 'одиннадцать', ',', 'пять', 'и', 'шесть', '.', 'она', 'уметь', 'угадывать', 'все', 'номер', 'победитель', 'и', 'становиться', 'обладательница', 'джекпот', 'в', 'xx', 'xxx', 'доллар', '(', '885,3', 'тысяча', 'рубль', ')', '.', 'ранее', 'сообщать', 'о', 'американец', ',', 'который', 'срывать', 'джекпот', ',', 'подражать', 'герой', 'книга', ',', 'писать', 'другой', '.', 'персонаж', 'каждый', 'утро', 'пила', 'кофе', 'в', 'торговый', 'центр', 'и', 'покупать', 'билет', 'мгновенный', 'лотерея', '.', 'читатель', 'делать', 'так', 'же', 'и', 'выигрывать', 'xx', 'тысяча', 'доллар', '(', '3,3', 'миллион', 'рубль', ')', '.'], ['O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[333].__dict__.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ff321-d408-4daf-895e-3d16aa87fe50",
   "metadata": {},
   "source": [
    "Теперь составим словари слов и токен с помощью `Field` класса, а ещё сохраним индексы \"пустого\" слова и тэга. Этими \"пустыми\" словами и тэгами будут \"дозабиваться\" матрички документа в батче, чтобы каждый документ из батча был одного размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02717a4f-ad4f-4e96-a01a-cd3c75b8ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fields to vocabulary list\n",
    "min_word_freq = 3\n",
    "batch_size = 10\n",
    "word_field.build_vocab(train_data.word, min_freq=min_word_freq)\n",
    "tag_field.build_vocab(train_data.tag)\n",
    "\n",
    "# prepare padding index to be ignored during model training/evaluation\n",
    "word_pad_idx = word_field.vocab.stoi[word_field.pad_token]\n",
    "tag_pad_idx = tag_field.vocab.stoi[tag_field.pad_token]\n",
    "\n",
    "# # create iterator for batch input\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_data, val_data, test_data),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffad9387-8ddd-4c53-81a1-223d7b4ae3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty word index: 1\n",
      "Empty tag index: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Empty word index: {word_pad_idx}\")\n",
    "print(f\"Empty tag index: {tag_pad_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d76ebf74-339c-436e-aa45-d7b6a82c0ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 5000 sentences\n",
      "Val set: 500 sentences\n",
      "Test set: 1000 sentences\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set: {len(train_data)} sentences\")\n",
    "print(f\"Val set: {len(val_data)} sentences\")\n",
    "print(f\"Test set: {len(test_data)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d2b9b-2329-4b50-b5e1-7e83bb5a69e0",
   "metadata": {},
   "source": [
    "Чтобы посмотреть, что в батче, можно раскомментровать и запустить код ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b59c7665-7f7e-4eb4-82d6-810fdba390cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_iter:\n",
    "#     print(batch)\n",
    "#     print(batch.word)\n",
    "#     print(batch.tag)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63478f0-5fc3-4544-abda-4ae26e7c81b2",
   "metadata": {},
   "source": [
    "Обернём всё в один класс, чтобы было проще его перекладывать туда-сюда во время тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99986935-bfb7-4744-b795-306c111ab3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DataIterator:\n",
    "    train_iter: BucketIterator\n",
    "    val_iter: BucketIterator\n",
    "    test_iter: BucketIterator\n",
    "    tag_pad_idx: int\n",
    "    word_pad_idx: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4a2f7-7ae1-4e82-bd4b-7bc63197a7c5",
   "metadata": {},
   "source": [
    "## Моделька"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c055df-f7bf-424b-ae1c-a300501f2ed9",
   "metadata": {},
   "source": [
    "В этот раз  натренируем свои эмбэдинги и засунем их в bidirectional LSTM.\n",
    "\n",
    "Моделька будет состоять из:\n",
    "1. Embedding слоя ([nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))\n",
    "1. BiLSTM слоёв ([nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html))\n",
    "1. Линейного слоя, чтобы сделать классификацию\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d911696-7218-4967-a543-3f432767dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, lstm_layers,\n",
    "               emb_dropout, lstm_dropout, fc_dropout, word_pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # LAYER 1: Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim, \n",
    "            embedding_dim=embedding_dim, \n",
    "            padding_idx=word_pad_idx\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout) # TODO: we can add dropout in v2\n",
    "        # LAYER 2: BiLSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        # LAYER 3: Fully-connected layer\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # times 2 for bidirectional\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # sentence = [sentence length, batch size]\n",
    "        # embedding_out = [sentence length, batch size, embedding dim]\n",
    "        embedding_out = self.emb_dropout(self.embedding(sentence))\n",
    "        # lstm_out = [sentence length, batch size, hidden dim * 2]\n",
    "        lstm_out, _ = self.lstm(embedding_out)\n",
    "        # ner_out = [sentence length, batch size, output dim]\n",
    "        ner_out = self.fc(self.fc_dropout(lstm_out))\n",
    "        return ner_out\n",
    "\n",
    "    def init_weights(self):\n",
    "        # to initialize all parameters from normal distribution\n",
    "        # helps with converging during training\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "    def init_embeddings(self, word_pad_idx):\n",
    "        # initialize embedding for padding as zero\n",
    "        self.embedding.weight.data[word_pad_idx] = torch.zeros(self.embedding_dim)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8aa5c6a9-2ccf-4f24-a973-e5a1dc8ae187",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = BiLSTM(\n",
    "    input_dim=len(word_field.vocab),\n",
    "    embedding_dim=120, # 300\n",
    "    hidden_dim=64,\n",
    "    output_dim=len(tag_field.vocab),\n",
    "    lstm_layers=2,\n",
    "    emb_dropout=0.5,\n",
    "    lstm_dropout=0.1,\n",
    "    fc_dropout=0.25,\n",
    "    word_pad_idx=word_pad_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c144e245-fec8-491e-a4e7-4b95b987e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,323,192 trainable parameters.\n",
      "BiLSTM(\n",
      "  (embedding): Embedding(17730, 120, padding_idx=1)\n",
      "  (emb_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (lstm): LSTM(120, 64, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "  (fc_dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bilstm.init_weights()\n",
    "bilstm.init_embeddings(word_pad_idx=word_pad_idx)\n",
    "print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n",
    "print(bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ca0d9-9838-4e99-8f11-3e592bd2a079",
   "metadata": {},
   "source": [
    "## Тренировка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb73a41-4706-4502-a7ba-7238d4ea5e35",
   "metadata": {},
   "source": [
    "Ниже будет много функций для тренировки и валидации модельки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd49a9ed-402f-4d71-98ec-22a4f2db8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852bfbb-4fd8-4d64-9ee5-08fa7dfdaa33",
   "metadata": {},
   "source": [
    "(Хорошо, что cuda отвалилась после тренировки, а не до 🙃) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c22b9b-ffb2-4a1e-b0ca-6f3e3d1a8c6c",
   "metadata": {},
   "source": [
    "Простая функция, что считать время одной эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "136d86ae-88ef-475e-8b3e-ecd5952084bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a6099-8e0a-4c21-985b-0791a4108ec4",
   "metadata": {},
   "source": [
    "Метрика качества классификации. Будет использовать обычную accuracy, но с дополнительными фишками для учёта паддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc776c3-498d-4b64-9cd9-1094439b2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y, device):\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True).to(device)  # get the index of the max probability\n",
    "    non_pad_elements = (y != data.tag_pad_idx).nonzero().to(device)  # prepare masking for paddings\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements]).to(device)\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc444cd-4075-472a-9660-207503d68e4a",
   "metadata": {},
   "source": [
    "Функция для запуска одной эпохи, всё довольно стандартно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb48e972-1c0e-4842-a322-887ebb1c1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, data, optimizer, loss_fn, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in data.train_iter:\n",
    "        # text = [sent len, batch size]\n",
    "        text = batch.word.to(device)\n",
    "        # tags = [sent len, batch size]\n",
    "        true_tags = batch.tag.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_tags = model(text)\n",
    "        # to calculate the loss and accuracy, we flatten both prediction and true tags\n",
    "        # flatten pred_tags to [sent len, batch size, output dim]\n",
    "        pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "        # flatten true_tags to [sent len * batch size]\n",
    "        true_tags = true_tags.view(-1)\n",
    "        batch_loss = loss_fn(pred_tags, true_tags)\n",
    "        batch_acc = accuracy(pred_tags, true_tags, device)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "        epoch_acc += batch_acc.item()\n",
    "    return epoch_loss / len(data.train_iter), epoch_acc / len(data.train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd8953-0640-463c-a59f-ab981454be31",
   "metadata": {},
   "source": [
    "Функция для валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad32b69b-8423-4619-b282-a93616c4d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      # similar to epoch() but model is in evaluation mode and no backprop\n",
    "        for batch in iterator:\n",
    "            text = batch.word.to(device)\n",
    "            true_tags = batch.tag.to(device)\n",
    "            pred_tags = model(text)\n",
    "            pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "            true_tags = true_tags.view(-1)\n",
    "            batch_loss = loss_fn(pred_tags, true_tags)\n",
    "            batch_acc = accuracy(pred_tags, true_tags, device)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b97091-6477-495b-9aaa-3fd846e5379b",
   "metadata": {},
   "source": [
    "И, наконец, тренировка "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc4daf94-a6e9-4057-bf1e-03b1ec751a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn, device, n_epochs, n_val=5):\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = run_epoch(model, data, optimizer, loss_fn,device)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print(f\"Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(f\"\\tTrn Loss: {train_loss:.3f} | Trn Acc: {train_acc * 100:.2f}%\")\n",
    "        if epoch % n_val == 0:\n",
    "            val_loss, val_acc = evaluate(model, data.val_iter, loss_fn, device)\n",
    "            print(f\"\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc * 100:.2f}%\")\n",
    "    test_loss, test_acc = evaluate(model, data.test_iter, loss_fn, device)\n",
    "    print(f\"Test Loss: {test_loss:.3f} |  Test Acc: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b9b95-80b3-480b-9e22-2b7c52d5350e",
   "metadata": {},
   "source": [
    "Инициализируем и запускаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7cb86a7-5ae6-46dd-a92b-819447ea1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=bilstm.to(device)\n",
    "data= DataIterator(train_iter=train_iter, val_iter=val_iter, test_iter=test_iter,tag_pad_idx=tag_pad_idx,word_pad_idx=word_pad_idx)\n",
    "optimizer=torch.optim.Adam(model.parameters())\n",
    "loss_fn=nn.CrossEntropyLoss(ignore_index=data.tag_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95f8a2a3-5f83-4e5f-b8f4-0f2d69b6dfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 16s\n",
      "\tTrn Loss: 0.313 | Trn Acc: 91.71%\n",
      "\tVal Loss: 0.118 | Val Acc: 96.33%\n",
      "Epoch: 02 | Epoch Time: 1m 14s\n",
      "\tTrn Loss: 0.100 | Trn Acc: 96.88%\n",
      "Epoch: 03 | Epoch Time: 1m 14s\n",
      "\tTrn Loss: 0.068 | Trn Acc: 97.86%\n",
      "Epoch: 04 | Epoch Time: 1m 15s\n",
      "\tTrn Loss: 0.055 | Trn Acc: 98.26%\n",
      "Epoch: 05 | Epoch Time: 1m 16s\n",
      "\tTrn Loss: 0.046 | Trn Acc: 98.53%\n",
      "Epoch: 06 | Epoch Time: 1m 17s\n",
      "\tTrn Loss: 0.041 | Trn Acc: 98.70%\n",
      "\tVal Loss: 0.060 | Val Acc: 98.14%\n",
      "Epoch: 07 | Epoch Time: 1m 18s\n",
      "\tTrn Loss: 0.035 | Trn Acc: 98.87%\n",
      "Epoch: 08 | Epoch Time: 1m 15s\n",
      "\tTrn Loss: 0.032 | Trn Acc: 98.95%\n",
      "Epoch: 09 | Epoch Time: 1m 15s\n",
      "\tTrn Loss: 0.029 | Trn Acc: 99.06%\n",
      "Epoch: 10 | Epoch Time: 1m 16s\n",
      "\tTrn Loss: 0.027 | Trn Acc: 99.13%\n",
      "Test Loss: 0.075 |  Test Acc: 97.93%\n"
     ]
    }
   ],
   "source": [
    "train(model, data, optimizer, loss_fn, device, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f409ba4-728c-458a-95a4-7a5a0d1eb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ner-bilstm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e540efe-4636-4d1f-87eb-d3394c0e7964",
   "metadata": {},
   "source": [
    "Кажется, что-то даже получилось 🤓 Тестовая accuracy 97.93% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
